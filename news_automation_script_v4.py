import feedparser
import requests
import json
import re
import openai
import os.path
import datetime
import smtplib
import getpass
import urllib.parse
import time
import warnings
import urllib3
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.utils import formatdate
from email.header import Header
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup # âœ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# ==============================================================================
# --- 1. ì‚¬ìš©ì ì„¤ì • (GitHub Actions Secretsì—ì„œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤) ---
# ==============================================================================

# Google Alerts RSS ì£¼ì†Œ
GOOGLE_ALERTS_RSS_URLS = [
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/2091321787487599294", #Satellite Communications
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/7282625974461397688", #ìœ„ì„±í†µì‹ 
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/2091321787487600193", #Non terrestrial Networks
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/2091321787487600258", #3GPP
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/6144919849490706746", #6G
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/13972650129806487379", #ì €ê¶¤ë„
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/5231113795348014351", #FCC 47 CFR PArt 25
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/6144919849490708240", #low Earth orbit
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/12348804382892789873", #ì£¼íŒŒìˆ˜
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/6144919849490708655", #AI-RAN
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/270492137594840372", #AI-RAN
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/2496376606356182211", #AI network
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/2496376606356181274", #ITU-R
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/18373922797329225191", #ISAC
    "https://www.google.co.kr/alerts/feeds/14299983816346888060/2496376606356184244", #IMT-2030
]

# Naver ê²€ìƒ‰ í‚¤ì›Œë“œ
NAVER_QUERIES = [
    "ìœ„ì„±í†µì‹ ", "satellite communication",
    "ì €ê¶¤ë„", "LEO",
    "ICT í‘œì¤€", "ICT standardization",
    "ì£¼íŒŒìˆ˜ ì •ì±…", "spectrum policy",
    "3GPP", "ITU", "FCC", "ofcom"
]

# GitHub Secretsë¥¼ í†µí•´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ì™€ ì„¤ì •ê°’ ë¶ˆëŸ¬ì˜¤ê¸°
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
GMAIL_PASSWORD = os.environ.get("GMAIL_PASSWORD")
RECEIVER_EMAIL = [email.strip() for email in os.environ.get("RECEIVER_EMAIL", "").split(',') if email.strip()]

# Google API ì„¤ì •
SCOPES = ['https://www.googleapis.com/auth/documents', 'https://www.googleapis.com/auth/drive']

# ==============================================================================
# --- 1. í—¬í¼ í•¨ìˆ˜ (âœ¨ ìƒˆë¡œì›Œì§„ ë²„ì „) ---
# ==============================================================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def configure_ssl_warnings(suppress_warnings=True):
    """
    SSL ê´€ë ¨ ê²½ê³ ë¥¼ ì œì–´í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        suppress_warnings (bool): Trueë©´ ê²½ê³  ì–µì œ, Falseë©´ ê²½ê³  í‘œì‹œ
    """
    if suppress_warnings:
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        warnings.filterwarnings('ignore', message='Unverified HTTPS request')
    else:
        # ê²½ê³  ë‹¤ì‹œ í™œì„±í™”í•˜ë ¤ë©´
        warnings.resetwarnings()

def extract_google_alerts_url(google_url: str) -> str:
    """
    êµ¬ê¸€ ì•Œë¦¬ë¯¸ RSSì˜ ë³µì¡í•œ ë§í¬ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        google_url (str): êµ¬ê¸€ ì•Œë¦¬ë¯¸ì—ì„œ ì œê³µí•˜ëŠ” ì›ë³¸ ë§í¬
        
    Returns:
        str: ì¶”ì¶œëœ ì‹¤ì œ ë‰´ìŠ¤ URL
    """
    try:
        # ë°©ë²• 1: &url= íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
        if "&url=" in google_url:
            extracted_url = google_url.split("&url=")[1]
            # URL ë””ì½”ë”©
            extracted_url = urllib.parse.unquote(extracted_url)
            # ì¶”ê°€ íŒŒë¼ë¯¸í„° ì œê±° (&sa=U ë“±)
            if "&" in extracted_url:
                extracted_url = extracted_url.split("&")[0]
            return extracted_url
        
        # ë°©ë²• 2: q= íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ (êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ í˜•íƒœ)
        if "q=" in google_url:
            parsed = urlparse(google_url)
            query_params = parse_qs(parsed.query)
            if 'q' in query_params:
                potential_url = query_params['q'][0]
                if potential_url.startswith('http'):
                    return potential_url
        
        # ë°©ë²• 3: ì§ì ‘ HTTPì¸ ê²½ìš°
        if google_url.startswith('http') and 'google.com' not in google_url:
            return google_url
            
        return google_url  # ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        
    except Exception as e:
        print(f"    (ê²½ê³ ) URL ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
        return google_url

def get_final_url_and_source(url: str, max_retries: int = 2) -> tuple:
    """
    ë¦¬ë””ë ‰ì…˜ì„ ë”°ë¼ê°€ ìµœì¢… URLì„ ì°¾ê³  ì¶œì²˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    
    Args:
        url (str): ì¶”ì í•  URL
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        tuple: (ìµœì¢… URL, ì¶”ì¶œëœ ì–¸ë¡ ì‚¬ ì´ë¦„, ì„±ê³µ ì—¬ë¶€)
    """
    for attempt in range(max_retries + 1):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ë” ì§§ê²Œ (SSL ê²€ì¦ ì‹œë„ í›„ ì‹¤íŒ¨ì‹œ ë¹„í™œì„±í™”)
            try:
                # ë¨¼ì € SSL ê²€ì¦ í™œì„±í™”ë¡œ ì‹œë„
                response = requests.get(url, headers=headers, allow_redirects=True, 
                                        timeout=(5, 10), verify=True)
            except requests.exceptions.SSLError:
                # SSL ì˜¤ë¥˜ ì‹œ ê²€ì¦ ë¹„í™œì„±í™”ë¡œ ì¬ì‹œë„
                response = requests.get(url, headers=headers, allow_redirects=True, 
                                        timeout=(5, 10), verify=False)
            
            # ìƒíƒœ ì½”ë“œ ì²´í¬ (404, 403 ë“±ë„ í—ˆìš©í•˜ë˜ ê¸°ë¡)
            if response.status_code >= 400:
                print(f"    (ì •ë³´) HTTP {response.status_code}: {url[:60]}...")
                # ê·¸ë˜ë„ URL íŒŒì‹±ì€ ì‹œë„
                
            final_url = response.url
            parsed_url = urlparse(final_url)
            domain = parsed_url.netloc
            
            # ë„ë©”ì¸ì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì¶œ (ë” ì •êµí•˜ê²Œ)
            domain_clean = domain.replace('www.', '').replace('m.', '')
            source_parts = domain_clean.split('.')
            
            # í•œêµ­ ì–¸ë¡ ì‚¬ ë„ë©”ì¸ íŠ¹ë³„ ì²˜ë¦¬
            source_mapping = {
                'chosun': 'ì¡°ì„ ì¼ë³´', 'donga': 'ë™ì•„ì¼ë³´', 'joongang': 'ì¤‘ì•™ì¼ë³´',
                'hankyoreh': 'í•œê²¨ë ˆ', 'hani': 'í•œê²¨ë ˆ', 'khan': 'ê²½í–¥ì‹ ë¬¸',
                'mt': 'ë¨¸ë‹ˆíˆ¬ë°ì´', 'mk': 'ë§¤ì¼ê²½ì œ', 'seoul': 'ì„œìš¸ì‹ ë¬¸',
                'ytn': 'YTN', 'sbs': 'SBS', 'kbs': 'KBS', 'mbc': 'MBC'
            }
            
            source_name = source_mapping.get(source_parts[0].lower(), source_parts[0].capitalize())
            
            return final_url, source_name, True
            
        except requests.exceptions.Timeout:
            print(f"    (ì¬ì‹œë„ {attempt + 1}/{max_retries + 1}) íƒ€ì„ì•„ì›ƒ: {url[:50]}...")
            if attempt < max_retries:
                time.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue
                
        except requests.exceptions.RequestException as e:
            error_msg = str(e)[:100]
            print(f"    (ì¬ì‹œë„ {attempt + 1}/{max_retries + 1}) ìš”ì²­ ì˜¤ë¥˜: {error_msg}")
            if attempt < max_retries:
                time.sleep(1)
                continue
                
        except Exception as e:
            print(f"    (ê²½ê³ ) ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)[:100]}")
            break
    
    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ URLì—ì„œ ë„ë©”ì¸ë§Œ ì¶”ì¶œí•´ì„œë¼ë„ ì†ŒìŠ¤ ì´ë¦„ ìƒì„±
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '').replace('m.', '')
        fallback_source = domain.split('.')[0].capitalize() if domain else "ì¶œì²˜ ë¶ˆëª…"
        return url, fallback_source, False
    except:
        return url, "ì¶œì²˜ ë¶ˆëª…", False
        

# ğŸ’¡ğŸ’¡ğŸ’¡ --- [ì‹ ê·œ] ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ --- ğŸ’¡ğŸ’¡ğŸ’¡
def get_article_content(url: str, max_length: int = 5000) -> str:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        url (str): ë‰´ìŠ¤ ê¸°ì‚¬ URL
        max_length (int): API í† í° ì œí•œì„ ìœ„í•´ ê°€ì ¸ì˜¬ ìµœëŒ€ ê¸€ì ìˆ˜
        
    Returns:
        str: ì¶”ì¶œ ë° ì •ì œëœ ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        response.raise_for_status()

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'lxml')

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³  ë“±)
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()

        # ê¸°ì‚¬ ë³¸ë¬¸ ìœ ë ¥ í›„ë³´ íƒœê·¸ íƒìƒ‰
        article_body = soup.find('article') or \
                       soup.find('div', id=re.compile(r'content|article|main', re.I)) or \
                       soup.find('main')
        
        if article_body:
            text = article_body.get_text(separator='\n', strip=True)
        else:
            # í›„ë³´ê°€ ì—†ìœ¼ë©´ ëª¨ë“  <p> íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¡°í•©
            paragraphs = soup.find_all('p')
            text = '\n'.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50)
            if not text: # ê·¸ë˜ë„ ì—†ìœ¼ë©´ body ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                 text = soup.body.get_text(separator='\n', strip=True)


        # í…ìŠ¤íŠ¸ ì •ì œ
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        cleaned_text = '\n'.join(chunk for chunk in chunks if chunk)
        
        if not cleaned_text:
            return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return cleaned_text[:max_length]

    except requests.exceptions.RequestException as e:
        return f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜): {e}"
    except Exception as e:
        return f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜): {e}"


# ==============================================================================
# --- 2. ê°œì„ ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ì˜¤ë¥˜ ì²˜ë¦¬ ë° í†µê³„ ì¶”ê°€) ---
# ==============================================================================

def get_news_data():
    """ì—¬ëŸ¬ RSS í”¼ë“œì™€ í‚¤ì›Œë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹¤ì œ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ëŠ” í•¨ìˆ˜"""
    news_list = []
    failed_urls = []  # ì‹¤íŒ¨í•œ URLë“¤ ì¶”ì 
    
    # í†µê³„ ì¶”ì ìš©
    stats = {
        'google_alerts': {'total': 0, 'success': 0, 'failed': 0, 'connection_errors': 0},
        'naver': {'total': 0, 'success': 0, 'failed': 0, 'connection_errors': 0}
    }
    
    print("\nğŸ” Google Alertsì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    
    for i, rss_url in enumerate(GOOGLE_ALERTS_RSS_URLS, 1):
        if not rss_url.strip(): 
            continue
            
        print(f"  ğŸ“¡ RSS í”¼ë“œ {i}/{len(GOOGLE_ALERTS_RSS_URLS)} ì²˜ë¦¬ ì¤‘...")
        
        try:
            # RSS íŒŒì‹± ìì²´ë„ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            import socket
            old_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(15)  # RSS íŒŒì‹± íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¼
            
            feed = feedparser.parse(rss_url)
            socket.setdefaulttimeout(old_timeout)
            
            if not hasattr(feed, 'entries') or not feed.entries:
                print(f"    âš ï¸  RSS í”¼ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨")
                continue
                
            print(f"    ğŸ“° {len(feed.entries)}ê°œ í•­ëª© ë°œê²¬")
            
            # ê° í•­ëª©ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ì•ˆì •ì„± ìš°ì„ )
            for j, entry in enumerate(feed.entries, 1):
                stats['google_alerts']['total'] += 1
                
                print(f"        ğŸ”„ í•­ëª© {j}/{len(feed.entries)} ì²˜ë¦¬ ì¤‘...", end=' ')
                
                try:
                    # êµ¬ê¸€ ì•Œë¦¬ë¯¸ ë§í¬ì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
                    extracted_url = extract_google_alerts_url(entry.link)
                    
                    # URL ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ê¸´ URLì€ ê±´ë„ˆë›°ê¸°)
                    if len(extracted_url) > 500:
                        print("âŒ (URL ë„ˆë¬´ ê¹€)")
                        stats['google_alerts']['failed'] += 1
                        continue
                    
                    # ìµœì¢… URLê³¼ ì¶œì²˜ í™•ì¸
                    final_link, source, success = get_final_url_and_source(extracted_url)
                    
                    if success:
                        stats['google_alerts']['success'] += 1
                        print("âœ…")
                    else:
                        stats['google_alerts']['failed'] += 1
                        failed_urls.append(extracted_url)
                        # ì—°ê²° ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                        if "ì—°ê²°" in str(extracted_url) or "Connection" in str(extracted_url):
                            stats['google_alerts']['connection_errors'] += 1
                        print("âŒ")
                    
                    # ë°œí–‰ì¼ ì²˜ë¦¬ ê°œì„ 
                    try:
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            published_date = datetime.datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                        else:
                            # published_parsedê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë‚ ì§œ ì‚¬ìš©
                            published_date = datetime.datetime.now().strftime('%Y-%m-%d')
                    except Exception as date_error:
                        published_date = datetime.datetime.now().strftime('%Y-%m-%d')
                        print(f"         (ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_error})")
                    
                    news_list.append({
                        "title": entry.title,
                        "link": final_link,
                        "published": published_date,
                        "source": source,
                        "extraction_success": success
                    })
                    
                    # ê° ìš”ì²­ ì‚¬ì´ì— ì§§ì€ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    time.sleep(0.5)
                    
                except Exception as item_error:
                    stats['google_alerts']['failed'] += 1
                    failed_urls.append(getattr(entry, 'link', 'Unknown URL'))
                    print(f"âŒ (ì˜¤ë¥˜: {str(item_error)[:50]})")
                    continue
                    
        except Exception as feed_error:
            print(f"  âŒ RSS í”¼ë“œ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {str(feed_error)[:100]}")
            continue

    print(f"\nğŸ“Š Google Alerts í†µê³„:")
    print(f"    â€¢ ì´ ì²˜ë¦¬: {stats['google_alerts']['total']}ê°œ")
    print(f"    â€¢ ì„±ê³µ: {stats['google_alerts']['success']}ê°œ")
    print(f"    â€¢ ì‹¤íŒ¨: {stats['google_alerts']['failed']}ê°œ")
    if stats['google_alerts']['connection_errors'] > 0:
        print(f"    â€¢ ì—°ê²° ì˜¤ë¥˜: {stats['google_alerts']['connection_errors']}ê°œ")

    print("\nğŸ” Naver Newsì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    
    for i, query in enumerate(NAVER_QUERIES, 1):
        if not query.strip(): 
            continue
            
        print(f"  ğŸ” ê²€ìƒ‰ì–´ {i}/{len(NAVER_QUERIES)}: '{query}'")
        
        try:
            naver_url = "https://openapi.naver.com/v1/search/news.json"
            headers = {
                "X-Naver-Client-Id": NAVER_CLIENT_ID, 
                "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
            }
            params = {"query": query, "display": 20, "sort": "date"}
            
            response = requests.get(naver_url, headers=headers, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            items = data.get("items", [])
            print(f"    ğŸ“° {len(items)}ê°œ ë°œê²¬")
            
            for j, item in enumerate(items, 1):
                stats['naver']['total'] += 1
                
                print(f"        ğŸ”„ í•­ëª© {j}/{len(items)} ì²˜ë¦¬ ì¤‘...", end=' ')
                
                try:
                    clean_title = re.sub('<[^>]*>', '', item["title"])
                    
                    # ë‚ ì§œ íŒŒì‹± ê°œì„ 
                    try:
                        published_date = datetime.datetime.strptime(
                            item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900'
                        ).strftime('%Y-%m-%d')
                    except Exception as date_error:
                        published_date = datetime.datetime.now().strftime('%Y-%m-%d')
                
                    raw_link = item.get("originallink", item["link"])
                    
                    # URL ìœ íš¨ì„± ê¸°ë³¸ ì²´í¬
                    if not raw_link.startswith('http'):
                        print("âŒ (ì˜ëª»ëœ URL)")
                        stats['naver']['failed'] += 1
                        continue
                    
                    final_link, source, success = get_final_url_and_source(raw_link)
                    
                    if success:
                        stats['naver']['success'] += 1
                        print("âœ…")
                    else:
                        stats['naver']['failed'] += 1
                        failed_urls.append(raw_link)
                        print("âŒ")
                    
                    news_list.append({
                        "title": clean_title,
                        "link": final_link,
                        "published": published_date,
                        "source": source,
                        "extraction_success": success
                    })
                    
                    # ë„¤ì´ë²„ë„ ìš”ì²­ ê°„ ëŒ€ê¸°
                    time.sleep(0.3)
                    
                except Exception as item_error:
                    stats['naver']['failed'] += 1
                    print(f"âŒ (ì˜¤ë¥˜: {str(item_error)[:50]})")
                    continue
                    
        except Exception as e:
            print(f"  âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ API ì‹¤íŒ¨: {str(e)[:100]}")
            continue

    print(f"\nğŸ“Š Naver News í†µê³„:")
    print(f"    â€¢ ì´ ì²˜ë¦¬: {stats['naver']['total']}ê°œ")
    print(f"    â€¢ ì„±ê³µ: {stats['naver']['success']}ê°œ")
    print(f"    â€¢ ì‹¤íŒ¨: {stats['naver']['failed']}ê°œ")

    # ì‹¤íŒ¨í•œ URL ìƒìœ„ 5ê°œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    if failed_urls:
        print(f"\nâš ï¸  ì‹¤íŒ¨í•œ URL ìƒ˜í”Œ ({len(failed_urls)}ê°œ ì¤‘ ìµœëŒ€ 5ê°œ):")
        for i, failed_url in enumerate(failed_urls[:5], 1):
            print(f"    {i}. {failed_url[:80]}...")

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    print(f"\nğŸ”„ ì¤‘ë³µ ì œê±° ì „: {len(news_list)}ê°œ ë‰´ìŠ¤")
    
    news_list.sort(key=lambda x: x['published'], reverse=True)
    seen_links = set()
    unique_news_items = []
    
    for item in news_list:
        # URL ì •ê·œí™” ê°œì„ 
        try:
            normalized_link = re.sub(r'^https?:\/\/(www\.|m\.|amp\.)?', '', item['link']).rstrip('/')
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë„ ì œê±°í•˜ì—¬ ë” ì •í™•í•œ ì¤‘ë³µ ì œê±°
            normalized_link = normalized_link.split('?')[0].split('#')[0]
            
            if normalized_link not in seen_links:
                unique_news_items.append(item)
                seen_links.add(normalized_link)
        except Exception as e:
            # ì •ê·œí™” ì‹¤íŒ¨í•´ë„ ì¼ë‹¨ ì¶”ê°€
            unique_news_items.append(item)
            
    print(f"ğŸ¯ ì¤‘ë³µ ì œê±° í›„: {len(unique_news_items)}ê°œ ë‰´ìŠ¤")
    
    # ìµœì¢… ì„±ê³µë¥  ê³„ì‚° ë° ì¶œë ¥
    total_items = stats['google_alerts']['total'] + stats['naver']['total']
    total_success = stats['google_alerts']['success'] + stats['naver']['success']
    total_failed = stats['google_alerts']['failed'] + stats['naver']['failed']
    
    if total_items > 0:
        success_rate = (total_success / total_items * 100)
        print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
        print(f"    â€¢ ì „ì²´ ì‹œë„: {total_items}ê°œ")
        print(f"    â€¢ ì„±ê³µ: {total_success}ê°œ ({success_rate:.1f}%)")
        print(f"    â€¢ ì‹¤íŒ¨: {total_failed}ê°œ ({100-success_rate:.1f}%)")
        
        # ì„±ê³µë¥ ì´ ë‚®ìœ¼ë©´ ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        if success_rate < 70:
            print(f"\nğŸ’¡ ì„±ê³µë¥  ê°œì„  ì œì•ˆ:")
            print(f"    â€¢ VPN ì‚¬ìš© ê³ ë ¤")
            print(f"    â€¢ ì‹¤í–‰ ì‹œê°„ëŒ€ ë³€ê²½")
            print(f"    â€¢ RSS URL ì ê²€")
    
    return unique_news_items


# ==============================================================================
# --- 3. (ì‹ ê·œ) AI ë‰´ìŠ¤ ì„ ë³„ í•¨ìˆ˜ (ë¡œì§ êµ¬ì²´í™”) ---
# ==============================================================================
def filter_news_by_ai(news_items):
    """AIë¥¼ ì‚¬ìš©í•´ ì •ì±… ì…ì•ˆìì—ê²Œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ë¥¼ ì„ ë³„í•˜ëŠ” í•¨ìˆ˜"""
    print("\n[ğŸš€ ì‘ì—… ì¤‘] AIê°€ ì •ì±… ì…ì•ˆìë¥¼ ìœ„í•´ ë‰´ìŠ¤ë¥¼ ì„ ë³„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_OPENAI_API_KEY":
        print("  (ê²½ê³ ) OpenAI API í‚¤ê°€ ì—†ì–´ ë‰´ìŠ¤ ì„ ë³„ì„ ê±´ë„ˆë›°ê³  ìµœì‹  ë‰´ìŠ¤ 20ê°œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
        return news_items[:20]

    client = openai.OpenAI(api_key=OPENAI_API_KEY)

    formatted_news_list = ""
    for i, item in enumerate(news_items):
        formatted_news_list += f"{i}: {item['title']}\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ICT í‘œì¤€ ì •ì±… ìµœê³  ì „ë¬¸ê°€ì˜ ìˆ˜ì„ ë³´ì¢Œê´€ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë¨¼ì € ë‚´ìš©ì´ ì¤‘ë³µë˜ëŠ” ê¸°ì‚¬ë“¤ì„ ì œê±°í•œ ë’¤, 'í‘œì¤€ ì •ì±… ì…ì•ˆì'ì˜ ê´€ì ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 20ê°œë¥¼ ì„ ë³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì‘ì—… ì ˆì°¨]
    1. **ì¤‘ë³µ ì œê±°**: ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ì‚¬ì‹¤ìƒ ë™ì¼í•œ ì‚¬ê±´ì´ë‚˜ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ë“¤ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê³ , ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ í¬ê´„ì ì¸ ëŒ€í‘œ ê¸°ì‚¬ í•˜ë‚˜ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    2. **ìµœì¢… ì„ ë³„**: ì¤‘ë³µì´ ì œê±°ëœ ë‰´ìŠ¤ ëª©ë¡ì—ì„œ, ì•„ë˜ [ì„ ë³„ ìµœìš°ì„  ê¸°ì¤€]ì— ë”°ë¼ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 20ê°œë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì„ ë³„í•©ë‹ˆë‹¤.

    [ì„ ë³„ ìµœìš°ì„  ê¸°ì¤€]
    ì •ì±…ì  ì¤‘ìš”ë„ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ë©°, íŠ¹íˆ ì•„ë˜ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” êµ­ë‚´ì™¸ ë‰´ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    - **í•´ì™¸ ì£¼ìš”êµ­ ì •ì±…/ê·œì œ**: ë¯¸êµ­(FCC), ìœ ëŸ½(ETSI) ë“± í•´ì™¸ ì£¼ìš”êµ­ì˜ ICT ì •ì±…, ë²•ì•ˆ, ê·œì œ ë³€í™”
    - **êµ­ì œ í‘œì¤€í™” ë™í–¥**: 3GPP, ITU ë“± êµ­ì œ í‘œì¤€í™” ê¸°êµ¬ì˜ ì£¼ìš” ê²°ì • ë° ë…¼ì˜ ì‚¬í•­
    - **êµ­ë‚´ ì •ë¶€ ê³„íš ë° ë°œí‘œ**: êµ­ë‚´ ì •ë¶€ ë¶€ì²˜ê°€ ë°œí‘œí•˜ëŠ” ICT ì •ì±…, ë²•ì•ˆ, ê¸°ìˆ  ê°œë°œ ê³„íš
    - **ì‚°ì—…ê³„ í•µì‹¬ ë™í–¥**: ICT ì‚°ì—… ë° ì‹œì¥ íŒë„ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” êµ­ë‚´ì™¸ ê¸°ì—…ì˜ ê¸°ìˆ  ê°œë°œ ë° ì‚¬ì—… ì „ëµ
    - **ì •ì±… ë¹„íŒ ë° ëŒ€ì•ˆ**: í˜„ì¬ ì •ì±…ì˜ ë¬¸ì œì ì„ ì§€ì í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ëŒ€ì•ˆì„ ì œì‹œí•˜ëŠ” ê¸°ì‚¬


    [ë‰´ìŠ¤ ëª©ë¡]
    {formatted_news_list}

    [ìš”ì²­]
    ìœ„ ì ˆì°¨ì™€ ê¸°ì¤€ì— ë”°ë¼ ìµœì¢…ì ìœ¼ë¡œ ì„ ë³„ëœ ë‰´ìŠ¤ì˜ ë²ˆí˜¸ 20ê°œë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ë‹µí•´ ì£¼ì‹­ì‹œì˜¤.
    ì˜ˆì‹œ: 3, 8, 12, 15, 21, 23, 25, 30, 31, 33, 40, 41, 42, 45, 50
    (ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ë²ˆí˜¸ë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.)
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ICT í‘œì¤€ ì •ì±… ì „ë¬¸ê°€ì˜ ìœ ëŠ¥í•œ ë³´ì¢Œê´€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ê³ , ì •ì±…ì  ì¤‘ìš”ë„ê°€ ê°€ì¥ ë†’ì€ 20ê°œë¥¼ ê³¨ë¼ ë²ˆí˜¸ë§Œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
        )
        selected_indices_str = response.choices[0].message.content
        print(f"  > AIê°€ ì„ ë³„í•œ ë‰´ìŠ¤ ì¸ë±ìŠ¤: {selected_indices_str}")

        selected_indices = [int(i.strip()) for i in selected_indices_str.split(',')]
        
        filtered_news = [news_items[i] for i in selected_indices if i < len(news_items)]
        
        if not filtered_news:
             raise ValueError("AIê°€ ìœ íš¨í•œ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        return filtered_news

    except Exception as e:
        print(f"  (ê²½ê³ ) AI ë‰´ìŠ¤ ì„ ë³„ ì‹¤íŒ¨: {e}. ìµœì‹  ë‰´ìŠ¤ 20ê°œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return news_items[:20]

# ==============================================================================
# --- 4. AI ì‹¬ì¸µ ë¶„ì„ í•¨ìˆ˜ (í”„ë¡¬í”„íŠ¸ ìˆ˜ì •) ---
# ==============================================================================
def analyze_news_with_ai(news_item):
    """AIì—ê²Œ ë‰´ìŠ¤ë¥¼ ë³´ë‚´ ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜"""
    if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_OPENAI_API_KEY":
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."
        
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    
    # ğŸ’¡ğŸ’¡ğŸ’¡ --- [ìˆ˜ì •] í”„ë¡¬í”„íŠ¸ì— 'ë‰´ìŠ¤ ë³¸ë¬¸' ì¶”ê°€ --- ğŸ’¡ğŸ’¡ğŸ’¡
    prompt = f"""
    # Mission
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ 1ê°œë¥¼ ë¶„ì„í•˜ì—¬, ICT í‘œì¤€Â·ì •ì±… ì „ë¬¸ê°€ë¥¼ ìœ„í•œ 'ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ'ë¥¼ ìƒì„±í•˜ëŠ” AI ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë³´ê³ ì„œì˜ ëª¨ë“  ë‚´ìš©ì€ ë°˜ë“œì‹œ ê¸°ì‚¬ ë³¸ë¬¸ì— ëª…ì‹œëœ ì‚¬ì‹¤, ë°ì´í„°, ì¸ìš©ì— ê·¼ê±°í•´ì•¼ í•˜ë©°, ë‹¹ì‹ ì˜ ì‚¬ì „ ì§€ì‹ì´ë‚˜ ì™¸ë¶€ ì •ë³´ë¥¼ ì¶”ê°€í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ë¶„ì„ì€ ê¸°ì‚¬ì˜ ë‹¨í¸ì  ì •ë³´ë¥¼ ì—°ê²°í•˜ì—¬ ê¸°ìˆ , ì •ì±…, ì‹œì¥ ê´€ì ì˜ êµ¬ì²´ì ì¸ ì‹œì‚¬ì ì„ ë„ì¶œí•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.

    # Persona
    - **ì •ì²´ì„±:** 20ë…„ ê²½ë ¥ì˜ ICT í‘œì¤€Â·ì •ì±… ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸.
    - **ì „ë¬¸ì„±:** ê¸°ì‚¬ ì† ë°ì´í„°ì™€ ì¸ìš©ë¬¸ì„ ê·¼ê±°ë¡œ, ê¸°ìˆ ì Â·ì •ì±…ì Â·ì‹œì¥ì  ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ì‹¤ì§ˆì ì¸ íŒŒê¸‰íš¨ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ëŠ¥ìˆ™í•¨.
    - **í•µì‹¬ ì›ì¹™:** ì² ì €í•œ 'ê¸°ì‚¬ ê¸°ë°˜(Article-Based)' ë¶„ì„. ëª¨ë“  ë¶„ì„ê³¼ ì „ë§ì€ ê¸°ì‚¬ì˜ íŠ¹ì • ë¬¸ì¥ì´ë‚˜ ìˆ˜ì¹˜ì— ê¸°ë°˜í•˜ì—¬ ë…¼ë¦¬ë¥¼ ì „ê°œí•¨.

    # Process (Step-by-Step)
    1.  **[1ë‹¨ê³„: í•µì‹¬ ì •ë³´ ì¶”ì¶œ]**
    - ê¸°ì‚¬ì—ì„œ 'ëˆ„ê°€, ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„, ì–´ë–»ê²Œ, ì™œ'ì— í•´ë‹¹í•˜ëŠ” 6í•˜ ì›ì¹™ ê¸°ë°˜ì˜ í•µì‹¬ ì‚¬ì‹¤(fact)ì„ ëª¨ë‘ ì¶”ì¶œí•˜ì—¬ ëª©ë¡í™”í•©ë‹ˆë‹¤.
    - ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ëª¨ë“  êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, í†µê³„, ì¼ì •, ê³ ìœ ëª…ì‚¬(ì¸ë¬¼, ê¸°ì—…, ê¸°ê´€, ê¸°ìˆ ëª…)ë¥¼ ì •í™•íˆ ì‹ë³„í•©ë‹ˆë‹¤.
    - ì£¼ìš” ì´í•´ê´€ê³„ìë“¤ì˜ ë°œì–¸ì„ ì¸ìš©ë¬¸ í˜•íƒœë¡œ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    2.  **[2ë‹¨ê³„: ë¶„ì„ ë° ë³´ê³ ì„œ ì‘ì„±]**
    - ì•„ë˜ **[OUTPUT FORMAT]**ì— ì •ì˜ëœ êµ¬ì¡°ì— ë”°ë¼ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    - **[ì£¼ìš” ë‚´ìš© ìš”ì•½]** íŒŒíŠ¸: 1ë‹¨ê³„ì—ì„œ ì¶”ì¶œí•œ ê°ê´€ì  ì‚¬ì‹¤ë§Œì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤. ì–´ë– í•œ ì£¼ê´€ì  í•´ì„ì´ë‚˜ ì™¸ë¶€ ì •ë³´ë„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - **[ì‹œì‚¬ì  ë° ì „ë§]** íŒŒíŠ¸: **[ì£¼ìš” ë‚´ìš© ìš”ì•½]**ì—ì„œ ì •ë¦¬ëœ íŠ¹ì • ì‚¬ì‹¤ì´ë‚˜ ë°œì–¸ì„ ì§ì ‘ ì¸ìš©í•˜ë©°, ê·¸ê²ƒì´ ì™œ ì¤‘ìš”í•œì§€, ì–´ë–¤ êµ¬ì²´ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì¸ì§€ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤. "Aë¼ëŠ” ë°œì–¸ì€ Bë¼ëŠ” ê¸°ìˆ  í‘œì¤€ ë…¼ì˜ì— Cì™€ ê°™ì€ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒ"ê³¼ ê°™ì´ ëª…í™•í•˜ê²Œ ì„œìˆ í•©ë‹ˆë‹¤.
    - ë¬¸ì¥ì€ '~ë¡œ ë¶„ì„ë¨', '~ë¡œ íŒë‹¨ë¨', '~ë¥¼ ì‹œì‚¬í•¨' ë“± **ì „ë¬¸ê°€ì  íŒë‹¨ì„ ê°€ë¯¸í•œ ì„œìˆ í˜• ë¬¸ì²´**ë¡œ ì‘ì„±í•  ê²ƒ.

    # CONSTRAINTS
    - **ì—„ê²©í•œ ê·¼ê±° ì œì‹œ:** ëª¨ë“  ë¶„ì„ê³¼ ì „ë§ì€ "ê¸°ì‚¬ì— ë”°ë¥´ë©´...", "OOOì˜ ë°œì–¸ì„ í†µí•´ ë³¼ ë•Œ..." ì™€ ê°™ì´ ëª…í™•í•œ ê·¼ê±°ë¥¼ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
    - **ì¶”ë¡  ê¸ˆì§€:** ê¸°ì‚¬ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    - **êµ¬ì²´ì„±:** "í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒ"ê³¼ ê°™ì€ ì¶”ìƒì  í‘œí˜„ ëŒ€ì‹ , "ì–´ë–¤ ê°€ì¹˜ì‚¬ìŠ¬(e.g., ì¹©ì…‹, ë‹¨ë§, í”Œë«í¼)ì— ì–´ë–¤ ë³€í™”ë¥¼ ìœ ë°œí•  ê²ƒ"ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
    - **ì „ë¬¸ê°€ì  ë¬¸ì²´:** '~ë¡œ íŒë‹¨ë¨', '~ë¥¼ ì˜ë¯¸í•¨', '~ê°€ ì˜ˆìƒë¨' ë“± ì „ë¬¸ê°€ì˜ ë¶„ì„ì  ì–´ì¡°ë¥¼ ì¼ê´€ë˜ê²Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
        
    # Input Data
    - ë‰´ìŠ¤ ì œëª©: {news_item['title']}
    - ì›ë¬¸ ë§í¬: {news_item['link']}
    - ë‰´ìŠ¤ ë³¸ë¬¸:
    ---
    {news_item.get('content', 'ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.')}
    ---

    # OUTPUT FORMAT

    ## **ë‰´ìŠ¤ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ**

    ### **1. ì£¼ìš” ë‚´ìš© ìš”ì•½**
    ã…‡ [ê¸°ì‚¬ ë³¸ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ 1-2ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½. ëˆ„ê°€, ë¬´ì—‡ì„ í–ˆëŠ”ê°€ì— ì´ˆì ì„ ë§ì¶”ê³ , ë¬¸ì¥ì„ 'ã…‡'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ì‘ì„±]
    ã…‡ [ê¸°ì‚¬ì— ë‚˜íƒ€ë‚œ ì‚¬ê±´ì˜ ë°°ê²½ê³¼ ì›ì¸ì„ ê°ê´€ì ìœ¼ë¡œ ì„œìˆ . ë¬¸ì¥ì„ 'ã…‡'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ì‘ì„±]
    ã…‡ [ê¸°ì‚¬ì— ì–¸ê¸‰ëœ í•µì‹¬ ìˆ˜ì¹˜, ì¼ì •, ë°ì´í„°ë¥¼ ì¸ìš©í•˜ê³  ê·¸ê²ƒì´ ì˜ë¯¸í•˜ëŠ” íŒ©íŠ¸ë¥¼ ì„¤ëª…. ë¬¸ì¥ì„ 'ã…‡'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ì‘ì„±]
    ã…‡ [ì£¼ìš” ì¸ë¬¼ ë˜ëŠ” ê¸°ê´€ì˜ ë°œì–¸ì´ë‚˜ ê³µì‹ ì…ì¥ì„ ì¸ìš©í•˜ì—¬ ì •ë¦¬. ë¬¸ì¥ì„ 'ã…‡'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ì‘ì„±]

    ### **2. ì‹œì‚¬ì  ë° ì „ë§**
    ã…‡ [ê¸°ì‚¬ ë‚´ìš©ì´ ICT ê¸°ìˆ , í‘œì¤€, ì •ì±…, ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì „ë§ì„ 'ã…‡'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ 1~2ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì„œìˆ ]
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ICT í‘œì¤€ ì •ì±… ë¶„ì„ ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ê¸°ì‚¬ ë³¸ë¬¸ë§Œì„ ê·¼ê±°ë¡œ 'ì£¼ìš” ë‚´ìš© ìš”ì•½'ê³¼ 'ì‹œì‚¬ì  ë° ì „ë§'ì„ ì‘ì„±í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3, max_tokens=1500, # í† í° ê¸¸ì´ ìƒí–¥
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"  (ê²½ê³ ) AI ì‹¬ì¸µ ë¶„ì„ ì‹¤íŒ¨ ({news_item['title']}): {e}")
        return "AI ì‹¬ì¸µ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

# ==============================================================================
# --- 5. êµ¬ê¸€ ë¬¸ì„œ ìƒì„± í•¨ìˆ˜ (ë””ìì¸ ê°œì„ ) ---
# ==============================================================================


def get_google_docs_service():
    """Google Docsì™€ Drive API ì„œë¹„ìŠ¤ë¥¼ ì¸ì¦í•˜ê³  ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    creds = None
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
            
    docs_service = build('docs', 'v1', credentials=creds)
    drive_service = build('drive', 'v3', credentials=creds)
    return docs_service, drive_service


def generate_google_doc_report(analyzed_data):
    try:
        docs_service, drive_service = get_google_docs_service()
    except FileNotFoundError:
        print("  (ì˜¤ë¥˜) 'credentials.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì¸ì¦ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    except Exception as e:
        print(f"  (ì˜¤ë¥˜) êµ¬ê¸€ ì„œë¹„ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None
        
    current_date = datetime.date.today().strftime('%Yë…„ %mì›” %dì¼')
    document_title = f"ì „íŒŒÂ·ì´ë™í†µì‹  ë™í–¥ ë³´ê³ ì„œ ({current_date})"

    try:
        # 1. ë¬¸ì„œ ìƒì„±
        document = docs_service.documents().create(body={'title': document_title}).execute()
        document_id = document.get('documentId')
        
        permission = {'type': 'anyone', 'role': 'reader'}
        drive_service.permissions().create(fileId=document_id, body=permission).execute()
        print("  > ë¬¸ì„œ ì ‘ê·¼ ê¶Œí•œì„ ê³µê°œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
        
        document_url = f"https://docs.google.com/document/d/{document_id}/edit"
        print(f"  > ìƒˆ ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {document_url}")

        # 2. ìŠ¤íƒ€ì¼ë§ëœ ë‚´ìš© ì¶”ê°€
        requests_list = []
        index = 1

        # --- ë¬¸ì„œ ì œëª© ìŠ¤íƒ€ì¼ë§ ---
        title_text = f"{document_title}\n"
        requests_list.append({'insertText': {'location': {'index': index}, 'text': title_text}})
        requests_list.append({'updateParagraphStyle': {'range': {'startIndex': index, 'endIndex': index + len(title_text)}, 'paragraphStyle': {'alignment': 'CENTER'}, 'fields': 'alignment'}})
        requests_list.append({'updateTextStyle': {'range': {'startIndex': index, 'endIndex': index + len(title_text) - 1}, 'textStyle': {'fontSize': {'magnitude': 18, 'unit': 'PT'}, 'bold': True}, 'fields': 'fontSize,bold'}})
        index += len(title_text)
        
        # --- AI ë¶„ì„ ê³ ì§€ ë¬¸êµ¬ ---
        disclaimer_text = "â€» ë³¸ ë³´ê³ ì„œì˜ ë‚´ìš©ì€ AIê°€ ìƒì„±í•œ ë¶„ì„ìœ¼ë¡œ, ê°œì¸ì ì¸ ì˜ê²¬ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
        requests_list.append({'insertText': {'location': {'index': index}, 'text': disclaimer_text}})
        requests_list.append({'updateParagraphStyle': {'range': {'startIndex': index, 'endIndex': index + len(disclaimer_text)}, 'paragraphStyle': {'alignment': 'CENTER'}, 'fields': 'alignment'}})
        requests_list.append({'updateTextStyle': {'range': {'startIndex': index, 'endIndex': index + len(disclaimer_text) - 2}, 'textStyle': {'fontSize': {'magnitude': 9, 'unit': 'PT'}, 'italic': True, 'foregroundColor': {'color': {'rgbColor': {'red': 0.5, 'green': 0.5, 'blue': 0.5}}}}, 'fields': 'fontSize,italic,foregroundColor'}})
        index += len(disclaimer_text)


        # --- ê° ë‰´ìŠ¤ ì•„ì´í…œ ìŠ¤íƒ€ì¼ë§ ---
        for i, data in enumerate(analyzed_data):
            # ë‰´ìŠ¤ ì œëª©
            news_title = f"[{i+1}] {data['title']}\n"
            requests_list.append({'insertText': {'location': {'index': index}, 'text': news_title}})
            requests_list.append({'updateTextStyle': {'range': {'startIndex': index, 'endIndex': index + len(news_title)}, 'textStyle': {'fontSize': {'magnitude': 14, 'unit': 'PT'}, 'bold': True}, 'fields': 'fontSize,bold'}})
            index += len(news_title)
            
            # ë©”íƒ€ë°ì´í„° (ì¶œì²˜, ë°œí–‰ì¼, ë§í¬)
            meta_text = f"ì¶œì²˜: {data['source']} | ë°œí–‰ì¼: {data['published']}\n"
            requests_list.append({'insertText': {'location': {'index': index}, 'text': meta_text}})
            requests_list.append({'updateTextStyle': {'range': {'startIndex': index, 'endIndex': index + len(meta_text)}, 'textStyle': {'fontSize': {'magnitude': 9, 'unit': 'PT'}, 'foregroundColor': {'color': {'rgbColor': {'red': 0.5, 'green': 0.5, 'blue': 0.5}}}}, 'fields': 'fontSize,foregroundColor'}})
            index += len(meta_text)
            
            link_text = f"ì›ë³¸ ë§í¬: {data['link']}\n\n"
            requests_list.append({'insertText': {'location': {'index': index}, 'text': link_text}})
            requests_list.append({'updateTextStyle': {'range': {'startIndex': index, 'endIndex': index + len(link_text)}, 'textStyle': {'fontSize': {'magnitude': 9, 'unit': 'PT'}, 'link': {'url': data['link']}}, 'fields': 'fontSize,link'}})
            index += len(link_text)

            # ë¶„ì„ ë‚´ìš© íŒŒì‹± (ì •ê·œì‹ ìˆ˜ì •)
            analysis_text = data.get('analysis_result', '')
            
            # ë³´ê³ ì„œ ì „ì²´ë¥¼ íŒŒì‹±
            report_match = re.search(r'## \*\*ë‰´ìŠ¤ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ\*\*(.*)', analysis_text, re.DOTALL)
            if report_match:
                report_content = report_match.group(1).strip()
            else:
                report_content = analysis_text # ë§¤ì¹˜ ì•ˆë˜ë©´ ê·¸ëƒ¥ ì „ì²´ ì‚¬ìš©
            
            # ì„¹ì…˜ ì œëª©ê³¼ ë‚´ìš©ì„ ë¶„ë¦¬í•˜ì—¬ ìŠ¤íƒ€ì¼ë§
            sections = re.split(r'### \*\*(.*?)\*\*', report_content)
            
            # sections[0]ì€ ë³´í†µ ë¹ˆ ë¬¸ìì—´
            for k in range(1, len(sections), 2):
                section_title = sections[k].strip() + "\n"
                section_body = sections[k+1].strip() + "\n\n"

                # ì„¹ì…˜ íƒ€ì´í‹€
                requests_list.append({'insertText': {'location': {'index': index}, 'text': section_title}})
                requests_list.append({'updateTextStyle': {'range': {'startIndex': index, 'endIndex': index + len(section_title)}, 'textStyle': {'bold': True}, 'fields': 'bold'}})
                
                # ë°°ê²½ìƒ‰
                if "ì£¼ìš” ë‚´ìš©" in section_title:
                    bg_color = {'red': 0.91, 'green': 0.95, 'blue': 1.0}
                elif "ì‹œì‚¬ì " in section_title:
                    bg_color = {'red': 1.0, 'green': 0.96, 'blue': 0.9}
                else:
                    bg_color = None

                if bg_color:
                    requests_list.append({'updateParagraphStyle': {'range': {'startIndex': index, 'endIndex': index + len(section_title)}, 'paragraphStyle': {'shading': {'backgroundColor': {'color': {'rgbColor': bg_color}}}}, 'fields': 'shading'}})
                index += len(section_title)
                
                # ì„¹ì…˜ ë³¸ë¬¸
                requests_list.append({'insertText': {'location': {'index': index}, 'text': section_body}})
                index += len(section_body)


        # 3. ì¼ê´„ ì—…ë°ì´íŠ¸ ì‹¤í–‰
        docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests_list}).execute()
        
        return document_url, document_title
    except Exception as e:
        print(f"  (ì˜¤ë¥˜) êµ¬ê¸€ ë¬¸ì„œ ìƒì„±/ìŠ¤íƒ€ì¼ë§ ì‹¤íŒ¨: {e}")
        return None, None


# ==============================================================================
# --- 6. Gmail ì „ì†¡ í•¨ìˆ˜ (í…œí”Œë¦¿ ë° íŒŒì‹± ë¡œì§ ìˆ˜ì •) ---
# ==============================================================================
def send_gmail_report(report_title, analyzed_data, doc_url, other_news):
    """ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒˆë¡œìš´ í˜•ì‹ì˜ ì´ë©”ì¼ë¡œ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜"""
    # 1. ì‹¬ì¸µ ë¶„ì„ëœ ë‰´ìŠ¤ HTML ìƒì„±
    news_items_html = ""
    for i, data in enumerate(analyzed_data):
        analysis_text = data.get('analysis_result', '')
        main_content = "ì£¼ìš”ë‚´ìš© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        implications = "ì‹œì‚¬ì  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ğŸ”§ ìˆ˜ì •ëœ íŒŒì‹± ë¡œì§ - ë‹¤ì–‘í•œ í˜•ì‹ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê°œì„ 
        try:
            print(f"  [ë””ë²„ê·¸] ë¶„ì„ í…ìŠ¤íŠ¸ ì¼ë¶€: {analysis_text[:200]}...")
            
            # íŒ¨í„´ 1: ### **1. ì£¼ìš” ë‚´ìš© ìš”ì•½** í˜•ì‹
            main_pattern1 = re.search(r'### \*\*1\. ì£¼ìš” ë‚´ìš© ìš”ì•½\*\*(.*?)### \*\*2\. ì‹œì‚¬ì  ë° ì „ë§\*\*', analysis_text, re.DOTALL)
            if main_pattern1:
                main_content = main_pattern1.group(1).strip()
                print(f"  [ë””ë²„ê·¸] íŒ¨í„´1ë¡œ ì£¼ìš”ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
            else:
                # íŒ¨í„´ 2: **1. ì£¼ìš” ë‚´ìš© ìš”ì•½** í˜•ì‹ (### ì—†ì´)
                main_pattern2 = re.search(r'\*\*1\. ì£¼ìš” ë‚´ìš© ìš”ì•½\*\*(.*?)\*\*2\. ì‹œì‚¬ì  ë° ì „ë§\*\*', analysis_text, re.DOTALL)
                if main_pattern2:
                    main_content = main_pattern2.group(1).strip()
                    print(f"  [ë””ë²„ê·¸] íŒ¨í„´2ë¡œ ì£¼ìš”ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                else:
                    # íŒ¨í„´ 3: 1. ì£¼ìš” ë‚´ìš© ìš”ì•½ í˜•ì‹ (ë³„í‘œ ì—†ì´)
                    main_pattern3 = re.search(r'1\. ì£¼ìš” ë‚´ìš© ìš”ì•½(.*?)2\. ì‹œì‚¬ì  ë° ì „ë§', analysis_text, re.DOTALL)
                    if main_pattern3:
                        main_content = main_pattern3.group(1).strip()
                        print(f"  [ë””ë²„ê·¸] íŒ¨í„´3ìœ¼ë¡œ ì£¼ìš”ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                    else:
                        print(f"  [ë””ë²„ê·¸] ì£¼ìš”ë‚´ìš© íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨")

            # ì‹œì‚¬ì  íŒŒì‹±ë„ ë™ì¼í•˜ê²Œ ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›
            # íŒ¨í„´ 1: ### **2. ì‹œì‚¬ì  ë° ì „ë§** í˜•ì‹
            impl_pattern1 = re.search(r'### \*\*2\. ì‹œì‚¬ì  ë° ì „ë§\*\*(.*)', analysis_text, re.DOTALL)
            if impl_pattern1:
                implications = impl_pattern1.group(1).strip()
                print(f"  [ë””ë²„ê·¸] íŒ¨í„´1ë¡œ ì‹œì‚¬ì  ì¶”ì¶œ ì„±ê³µ")
            else:
                # íŒ¨í„´ 2: **2. ì‹œì‚¬ì  ë° ì „ë§** í˜•ì‹
                impl_pattern2 = re.search(r'\*\*2\. ì‹œì‚¬ì  ë° ì „ë§\*\*(.*)', analysis_text, re.DOTALL)
                if impl_pattern2:
                    implications = impl_pattern2.group(1).strip()
                    print(f"  [ë””ë²„ê·¸] íŒ¨í„´2ë¡œ ì‹œì‚¬ì  ì¶”ì¶œ ì„±ê³µ")
                else:
                    # íŒ¨í„´ 3: 2. ì‹œì‚¬ì  ë° ì „ë§ í˜•ì‹
                    impl_pattern3 = re.search(r'2\. ì‹œì‚¬ì  ë° ì „ë§(.*)', analysis_text, re.DOTALL)
                    if impl_pattern3:
                        implications = impl_pattern3.group(1).strip()
                        print(f"  [ë””ë²„ê·¸] íŒ¨í„´3ìœ¼ë¡œ ì‹œì‚¬ì  ì¶”ì¶œ ì„±ê³µ")
                    else:
                        print(f"  [ë””ë²„ê·¸] ì‹œì‚¬ì  íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨")
            
            # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì •ë¦¬ (*, ** ì œê±°)
            main_content = re.sub(r'\*+', '', main_content).strip()
            implications = re.sub(r'\*+', '', implications).strip()
            
        except Exception as e:
            print(f"  (ê²½ê³ ) AI ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        news_items_html += f"""
        <div class="news-item">
            <div class="news-header">
                <h3 class="news-title">{data['title']}</h3>
                <div class="news-meta">
                    <span><strong>ì¶œì²˜:</strong> {data['source']}</span>
                    <span><strong>ë°œí–‰ì¼:</strong> {data['published']}</span>
                    <span><a href="{data['link']}" target="_blank">ì›ë¬¸ ê¸°ì‚¬ ë³´ê¸° &rarr;</a></span>
                </div>
            </div>
            <div class="analysis-container">
                <div class="analysis-section summary">
                    <div class="analysis-title"><span class="icon">ğŸ“</span><strong>ì£¼ìš” ë‚´ìš©</strong></div>
                    <p class="analysis-text">{main_content.replace('ã…‡', '&#8226;').replace('\n', '<br>')}</p>
                </div>
                <div class="analysis-section implications">
                    <div class="analysis-title"><span class="icon">ğŸ’¡</span><strong>ì‹œì‚¬ì  ë° ì „ë§</strong></div>
                    <p class="analysis-text">{implications.replace('ã…‡', '&#8226;').replace('\n', '<br>')}</p>
                </div>
            </div>
        </div>"""

    # 2. ê¸°íƒ€ ë‰´ìŠ¤ HTML ìƒì„± (ë³€ê²½ ì—†ìŒ)
    other_news_html = ""
    if other_news:
        other_news_html += """
        <div class="other-news-section">
            <h2>ê¸°íƒ€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤</h2>
            <ul class="other-news-list">
        """
        for item in other_news:
            other_news_html += f'<li><a href="{item["link"]}" target="_blank" class="other-news-link"><span class="other-news-title">{item["title"]}</span><span class="other-news-source">({item["source"]})</span></a></li>'
        
        other_news_html += "</ul></div>"


    # 3. ì „ì²´ ì´ë©”ì¼ ë³¸ë¬¸ ì¡°í•© (í…œí”Œë¦¿ ìˆ˜ì •)
    html_body = f"""
    <!DOCTYPE html>
    <html lang="ko"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>ICT ì£¼ìš”ê¸°ìˆ  ë™í–¥ ë¦¬í¬íŠ¸</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');
        body {{ margin: 0; padding: 0; background-color: #f4f7fa; font-family: 'Noto Sans KR', sans-serif; }}
        .email-container {{ max-width: 700px; margin: 20px auto; background-color: #ffffff; border-radius: 12px; overflow: hidden; box-shadow: 0 10px 40px rgba(0, 0, 0, 0.05); border: 1px solid #e9e9e9; }}
        .header {{ background: linear-gradient(135deg, #1D2B4A 0%, #2C3E6A 100%); color: #ffffff; padding: 40px; text-align: center; }}
        .header h1 {{ margin: 0; font-size: 28px; font-weight: 700; }} .header p {{ margin: 8px 0 0; font-size: 16px; font-weight: 300; opacity: 0.8; }}
        .disclaimer {{ font-size: 12px; opacity: 0.7; font-style: italic; margin-top: 15px;}}
        .main-content {{ padding: 40px; }}
        .report-intro {{ text-align: center; padding-bottom: 30px; border-bottom: 1px solid #eaeaea; margin-bottom: 30px; }}
        .button {{ display: inline-block; background: linear-gradient(135deg, #4A6DFF 0%, #6284FF 100%); color: #ffffff; padding: 14px 28px; border-radius: 50px; text-decoration: none; font-weight: 500; font-size: 15px; transition: all 0.3s ease; box-shadow: 0 4px 15px rgba(74, 109, 255, 0.3); }}
        .button:hover {{ transform: translateY(-2px); box-shadow: 0 8px 25px rgba(74, 109, 255, 0.4); }}
        .news-item {{ border: 1px solid #e9e9e9; border-radius: 12px; margin-bottom: 25px; overflow: hidden; transition: all 0.3s ease; }}
        .news-item:hover {{ transform: translateY(-2px); box-shadow: 0 8px 25px rgba(0, 0, 0, 0.07); }}
        .news-header {{ padding: 25px; }} .news-title {{ font-size: 20px; font-weight: 700; color: #1D2B4A; margin: 0 0 15px; }}
        .news-meta {{ font-size: 13px; color: #777; }} .news-meta a {{ color: #4A6DFF; text-decoration: none; font-weight: 500; }} .news-meta span {{ margin-right: 15px; }}
        .analysis-container {{ padding: 0 25px 25px 25px; border-top: 1px solid #e9e9e9; background-color: #f8f9fc; }}
        .analysis-section {{ padding: 20px; border-radius: 8px; margin-top: 15px; }}
        .analysis-section.summary {{ background-color: #e9f3ff; border-left: 4px solid #4A6DFF; }}
        .analysis-section.implications {{ background-color: #fff6e9; border-left: 4px solid #ff9f43; }}
        .analysis-title {{ display: flex; align-items: center; font-size: 16px; font-weight: 700; color: #1D2B4A; margin-bottom: 10px; }}
        .analysis-title .icon {{ font-size: 20px; margin-right: 10px; }}
        .analysis-text {{ font-size: 14px; line-height: 1.7; color: #333; margin: 0; }}
        .other-news-section {{ margin-top: 40px; padding-top: 30px; border-top: 1px solid #eaeaea; }}
        .other-news-section h2 {{ font-size: 20px; font-weight: 700; color: #1D2B4A; margin-bottom: 20px; text-align: center; }}
        .other-news-list {{ list-style-type: none; padding: 0; margin: 0; }}
        .other-news-list li {{ border-radius: 8px; margin-bottom: 5px; border-left: 3px solid #ccc; background-color: #f8f9fc; transition: background-color 0.2s ease; }}
        .other-news-list li:hover {{ background-color: #f1f3f8; }}
        .other-news-link {{ display: flex; justify-content: space-between; align-items: center; padding: 12px 15px; text-decoration: none; color: inherit; }}
        .other-news-title {{ color: #333; font-size: 14px; }}
        .other-news-source {{ color: #888; font-size: 12px; white-space: nowrap; margin-left: 15px; }}
        .footer {{ text-align: center; padding: 30px; background-color: #f4f7fa; font-size: 13px; color: #999; }}
    </style></head>
    <body><div class="email-container">
        <div class="header"><h1>{report_title}</h1><p class="disclaimer">â€» ë³¸ ë³´ê³ ì„œì˜ ë‚´ìš©ì€ IRONAGE AIê°€ ìƒì„±í•œ ë¶„ì„ìœ¼ë¡œ, ê°œì¸ì ì¸ ì˜ê²¬ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p></div>
        <div class="main-content">
            <div class="report-intro">
                <a href="{doc_url}" class="button" target="_blank">ğŸ“„ ì „ì²´ ë³´ê³ ì„œ ë³´ê¸°</a>
            </div>
            {news_items_html}
            {other_news_html}
        </div>
        <div class="footer"><p>ë³¸ ë¦¬í¬íŠ¸ëŠ” AI ê¸°ìˆ ì„ í™œìš©í•´ ìë™ ìƒì„±ëœ ë¶„ì„ ë³´ê³ ì„œì…ë‹ˆë‹¤.</p><p>Powered by Advanced IRONAGE AI Analytics</p></div>
    </div></body></html>"""

    msg = MIMEMultipart("alternative")
    msg["Subject"] = report_title
    msg["From"] = SENDER_EMAIL
    msg["To"] = ", ".join(RECEIVER_EMAIL)
    msg["Date"] = formatdate(localtime=True)
    msg.attach(MIMEText(html_body, 'html', 'utf-8'))
    
    try:
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(SENDER_EMAIL, GMAIL_PASSWORD)
        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())
        server.quit()
        print(f"  > âœ… ì´ë©”ì¼ì´ {', '.join(RECEIVER_EMAIL)} ì£¼ì†Œë¡œ ì„±ê³µì ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"  (ì˜¤ë¥˜) ì´ë©”ì¼ ë°œì†¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")


# ==============================================================================
# --- 7. ë””ë²„ê¹… í•¨ìˆ˜ ì¶”ê°€ ---
# ==============================================================================
def debug_analysis_parsing(analyzed_data):
    """AI ë¶„ì„ ê²°ê³¼ íŒŒì‹±ì„ ë””ë²„ê¹…í•˜ëŠ” í•¨ìˆ˜"""
    print("\nğŸ” [ë””ë²„ê·¸] AI ë¶„ì„ ê²°ê³¼ íŒŒì‹± í…ŒìŠ¤íŠ¸...")
    
    for i, data in enumerate(analyzed_data[:3]):  # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
        print(f"\n--- ë‰´ìŠ¤ {i+1}: {data['title'][:50]}... ---")
        analysis_text = data.get('analysis_result', '')
        
        if not analysis_text or analysis_text == "AI ì‹¬ì¸µ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.":
            print("âŒ AI ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŒ")
            continue
            
        print(f"ğŸ“„ AI ì‘ë‹µ ì „ì²´ ê¸¸ì´: {len(analysis_text)} ë¬¸ì")
        print(f"ğŸ“„ AI ì‘ë‹µ ì²« 300ì:")
        print(analysis_text[:300])
        print("...")
        
        # ì£¼ìš” íŒ¨í„´ë“¤ì„ í…ŒìŠ¤íŠ¸
        patterns_to_test = [
            (r'### \*\*1\. ì£¼ìš” ë‚´ìš© ìš”ì•½\*\*', "íŒ¨í„´1: ### **1. ì£¼ìš” ë‚´ìš© ìš”ì•½**"),
            (r'\*\*1\. ì£¼ìš” ë‚´ìš© ìš”ì•½\*\*', "íŒ¨í„´2: **1. ì£¼ìš” ë‚´ìš© ìš”ì•½**"),
            (r'1\. ì£¼ìš” ë‚´ìš© ìš”ì•½', "íŒ¨í„´3: 1. ì£¼ìš” ë‚´ìš© ìš”ì•½"),
            (r'## \*\*ë‰´ìŠ¤ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ\*\*', "íŒ¨í„´4: ## **ë‰´ìŠ¤ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ**"),
        ]
        
        print("ğŸ” íŒ¨í„´ ë§¤ì¹­ í…ŒìŠ¤íŠ¸:")
        for pattern, description in patterns_to_test:
            match = re.search(pattern, analysis_text)
            print(f"  {description}: {'âœ… ë°œê²¬' if match else 'âŒ ì—†ìŒ'}")


# ==============================================================================
# --- 8. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (ë””ë²„ê¹… ì¶”ê°€) ---
# ==============================================================================
if __name__ == "__main__":
    print("==============================================")
    print("AI ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("==============================================")
    
    print("\n[ì‘ì—… ì‹œì‘] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    unique_news_items = get_news_data()
    print(f"  > ì´ {len(unique_news_items)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

    # AIë¥¼ ì‚¬ìš©í•´ ì •ì±… ì…ì•ˆìì—ê²Œ ì¤‘ìš”í•œ ë‰´ìŠ¤ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    news_to_analyze = filter_news_by_ai(unique_news_items)
    print(f"  > AIê°€ ì„ ë³„í•œ {len(news_to_analyze)}ê°œì˜ í•µì‹¬ ë‰´ìŠ¤ë¥¼ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # ì„ ë³„ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ë‰´ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    analyzed_links = {item['link'] for item in news_to_analyze}
    other_news = [item for item in unique_news_items if item['link'] not in analyzed_links]


    analyzed_results = []
    if news_to_analyze:
        print("\n[ğŸš€ ì‘ì—… ì¤‘] ì„ íƒëœ ë‰´ìŠ¤ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for i, item in enumerate(news_to_analyze):
            print(f"  ({i+1}/{len(news_to_analyze)}) ë¶„ì„ ì¤‘: {item['title'][:40]}...")
            
            # ğŸ’¡ğŸ’¡ğŸ’¡ --- [ìˆ˜ì •] AI ë¶„ì„ ì „, ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘ ë‹¨ê³„ ì¶”ê°€ --- ğŸ’¡ğŸ’¡ğŸ’¡
            print(f"      -> ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
            item['content'] = get_article_content(item['link'])
            if "ì‹¤íŒ¨" in item['content'] or "ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" in item['content']:
                 print(f"      (ê²½ê³ ) {item['content']}")

            analysis = analyze_news_with_ai(item)
            item['analysis_result'] = analysis
            analyzed_results.append(item)

    # ğŸ” ë””ë²„ê¹… í•¨ìˆ˜ ì‹¤í–‰
    if analyzed_results:
        debug_analysis_parsing(analyzed_results)
        
        print("\n[ğŸš€ ì‘ì—… ì¤‘] êµ¬ê¸€ ë¬¸ì„œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        generated_doc_url, report_title = generate_google_doc_report(analyzed_results)

        if report_title:
            print("\n[ğŸš€ ì‘ì—… ì¤‘] ìƒì„±ëœ ë¦¬í¬íŠ¸ë¥¼ ì´ë©”ì¼ë¡œ ë°œì†¡í•©ë‹ˆë‹¤...")
            # 'other_news' ë¦¬ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬í•©ë‹ˆë‹¤.
            send_gmail_report(report_title, analyzed_results, generated_doc_url, other_news)

    print("\n==============================================")
    print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("==============================================")
